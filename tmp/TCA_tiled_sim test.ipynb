{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Systolic Array Component\n",
      "----------------------------------------\n",
      "Loaded (8, 4) weight matrix\n",
      "Cycle 1: Fed activations\n",
      "Cycle 2: Fed activations\n",
      "Cycle 3: Fed activations\n",
      "Collected 32 PSUMs\n",
      "First 4 PSUMs: [-4.289244331419468, 3.5882575171999633, 1.07267299387604, 13.036138594150543]\n",
      "Testing 32x32 Systolic Array Convolution Engine\n",
      "============================================================\n",
      "ConvEngine initialized:\n",
      "  Input: 64x64x4\n",
      "  Kernels: 8 kernels of size 3x3x4\n",
      "  Output: 62x62x8\n",
      "  Tile size: 8\n",
      "\n",
      "Starting convolution...\n",
      "\n",
      "Processing kernel group 1: kernels 0-7\n",
      "  Channel 1/4\n",
      "  Channel 2/4\n",
      "  Channel 3/4\n",
      "  Channel 4/4\n",
      "\n",
      "Convolution completed!\n",
      "\n",
      "Results:\n",
      "  Output shape: (62, 62, 8)\n",
      "  Output stats: mean=1.1287, std=34.0910\n",
      "  Output range: [-79.7243, 110.1866]\n",
      "\n",
      "Verification:\n",
      "  Expected output shape: (62, 62, 8)\n",
      "  Actual output shape: (62, 62, 8)\n",
      "  Shape match: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from typing import List, Tuple, Optional\n",
    "import math\n",
    "\n",
    "class PE:\n",
    "    \"\"\"Processing Element - individual unit in systolic array\"\"\"\n",
    "    def __init__(self):\n",
    "        self.weight = 0.0\n",
    "        self.activation = 0.0\n",
    "        self.accumulation = 0.0\n",
    "        \n",
    "    def load_weight(self, weight: float):\n",
    "        \"\"\"Load weight (stationary) into PE\"\"\"\n",
    "        self.weight = weight\n",
    "        \n",
    "    def feed_activation(self, activation: float) -> float:\n",
    "        \"\"\"Feed activation and return for next PE\"\"\"\n",
    "        self.activation = activation\n",
    "        return activation\n",
    "        \n",
    "    def cycle(self):\n",
    "        \"\"\"Compute MAC operation: accumulation += activation * weight\"\"\"\n",
    "        self.accumulation += self.activation * self.weight\n",
    "        \n",
    "    def get_psum(self) -> float:\n",
    "        \"\"\"Get accumulated partial sum\"\"\"\n",
    "        return self.accumulation\n",
    "        \n",
    "    def reset_accumulator(self):\n",
    "        \"\"\"Reset accumulator for next computation\"\"\"\n",
    "        self.accumulation = 0.0\n",
    "\n",
    "class SystolicArray:\n",
    "    \"\"\"32x32 Weight-Stationary Systolic Array\"\"\"\n",
    "    def __init__(self):\n",
    "        self.size = 32  # Fixed 32x32 array\n",
    "        # Grid of PEs: [row][col]\n",
    "        self.pes = [[PE() for _ in range(self.size)] for _ in range(self.size)]\n",
    "        \n",
    "    def load_weights(self, weight_matrix: np.ndarray):\n",
    "        \"\"\"Load weights in column-wise weight-stationary fashion\n",
    "        weight_matrix: (height, width) where each column is a flattened kernel\n",
    "        \"\"\"\n",
    "        height, width = weight_matrix.shape\n",
    "        assert height <= self.size and width <= self.size, f\"Weight matrix {height}x{width} exceeds SA size {self.size}x{self.size}\"\n",
    "        \n",
    "        # Load weights into PEs (stationary)\n",
    "        for row in range(height):\n",
    "            for col in range(width):\n",
    "                self.pes[row][col].load_weight(weight_matrix[row, col])\n",
    "                \n",
    "        # Zero out unused PEs\n",
    "        for row in range(height, self.size):\n",
    "            for col in range(width):\n",
    "                self.pes[row][col].load_weight(0.0)\n",
    "        for col in range(width, self.size):\n",
    "            for row in range(self.size):\n",
    "                self.pes[row][col].load_weight(0.0)\n",
    "                \n",
    "    def feed_activation_row(self, activation_row: List[float]):\n",
    "        \"\"\"Feed one row of activations into SA\"\"\"\n",
    "        # Extend activation row to SA width with zeros\n",
    "        extended_row = activation_row + [0.0] * (self.size - len(activation_row))\n",
    "        \n",
    "        # Feed activations to each column's first PE, then propagate downward\n",
    "        for col in range(self.size):\n",
    "            activation = extended_row[col]\n",
    "            \n",
    "            # Propagate activation down the column\n",
    "            for row in range(self.size):\n",
    "                activation = self.pes[row][col].feed_activation(activation)\n",
    "                \n",
    "    def cycle(self):\n",
    "        \"\"\"Execute one cycle: compute MAC operations in all PEs\"\"\"\n",
    "        for row in range(self.size):\n",
    "            for col in range(self.size):\n",
    "                self.pes[row][col].cycle()\n",
    "                \n",
    "    def collect_output(self) -> List[float]:\n",
    "        \"\"\"Collect PSUMs from each column (kernel outputs)\"\"\"\n",
    "        psums = []\n",
    "        for col in range(self.size):\n",
    "            col_psum = 0.0\n",
    "            for row in range(self.size):\n",
    "                col_psum += self.pes[row][col].get_psum()\n",
    "            psums.append(col_psum)\n",
    "        return psums\n",
    "        \n",
    "    def reset_accumulators(self):\n",
    "        \"\"\"Reset all PE accumulators\"\"\"\n",
    "        for row in range(self.size):\n",
    "            for col in range(self.size):\n",
    "                self.pes[row][col].reset_accumulator()\n",
    "\n",
    "class KernelLoader:\n",
    "    \"\"\"Prepares kernels for weight-stationary loading into 32x32 SA\"\"\"\n",
    "    def __init__(self, kernels: np.ndarray):\n",
    "        \"\"\"\n",
    "        kernels: (K, R, S, C) - K kernels, RxS spatial, C channels\n",
    "        \"\"\"\n",
    "        self.kernels = kernels\n",
    "        self.K, self.R, self.S, self.C = kernels.shape\n",
    "        self.flattened_size = self.R * self.S * self.C\n",
    "        \n",
    "    def get_kernel_matrix(self, kernel_group_start: int = 0, max_kernels: int = 32) -> np.ndarray:\n",
    "        \"\"\"Get weight matrix for SA loading\n",
    "        Returns: (height, width) matrix where each column is a flattened kernel\n",
    "        \"\"\"\n",
    "        num_kernels = min(max_kernels, self.K - kernel_group_start)\n",
    "        height = min(32, self.flattened_size)  # SA constraint\n",
    "        \n",
    "        weight_matrix = np.zeros((height, num_kernels))\n",
    "        \n",
    "        for k in range(num_kernels):\n",
    "            kernel_idx = kernel_group_start + k\n",
    "            if kernel_idx < self.K:\n",
    "                # Flatten kernel: (R, S, C) -> (R*S*C,)\n",
    "                flat_kernel = self.kernels[kernel_idx].flatten()\n",
    "                # Take only first 32 elements if kernel is too large\n",
    "                actual_size = min(len(flat_kernel), height)\n",
    "                weight_matrix[:actual_size, k] = flat_kernel[:actual_size]\n",
    "                \n",
    "        return weight_matrix\n",
    "        \n",
    "    def needs_vertical_tiling(self) -> bool:\n",
    "        \"\"\"Check if kernel needs vertical tiling\"\"\"\n",
    "        return self.flattened_size > 32\n",
    "        \n",
    "    def get_num_vertical_tiles(self) -> int:\n",
    "        \"\"\"Get number of vertical tiles needed\"\"\"\n",
    "        return math.ceil(self.flattened_size / 32)\n",
    "\n",
    "class ScratchpadMemory:\n",
    "    \"\"\"Local SRAM holding tiled portions of input feature map\"\"\"\n",
    "    def __init__(self, input_tensor: np.ndarray, tile_size: int, kernel_size: int, stride: int = 1):\n",
    "        self.input_tensor = input_tensor\n",
    "        self.H, self.W, self.C = input_tensor.shape\n",
    "        self.tile_size = tile_size\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        \n",
    "        # Calculate tile step and grid\n",
    "        self.tile_step = max(1, tile_size - kernel_size + 1)\n",
    "        self.tile_grid_h = math.ceil((self.H - kernel_size + 1) / self.tile_step)\n",
    "        self.tile_grid_w = math.ceil((self.W - kernel_size + 1) / self.tile_step)\n",
    "        \n",
    "    def generate_tile_addresses(self) -> List[Tuple[int, int]]:\n",
    "        \"\"\"Generate all tile start addresses\"\"\"\n",
    "        addresses = []\n",
    "        for tile_row in range(self.tile_grid_h):\n",
    "            for tile_col in range(self.tile_grid_w):\n",
    "                row_start = tile_row * self.tile_step\n",
    "                col_start = tile_col * self.tile_step\n",
    "                addresses.append((row_start, col_start))\n",
    "        return addresses\n",
    "        \n",
    "    def read_tile(self, tile_start: Tuple[int, int], channel: int) -> np.ndarray:\n",
    "        \"\"\"Read a tile from SPM with padding if necessary\"\"\"\n",
    "        row_start, col_start = tile_start\n",
    "        row_end = min(row_start + self.tile_size, self.H)\n",
    "        col_end = min(col_start + self.tile_size, self.W)\n",
    "        \n",
    "        # Extract tile\n",
    "        tile = self.input_tensor[row_start:row_end, col_start:col_end, channel]\n",
    "        \n",
    "        # Pad if necessary\n",
    "        if tile.shape[0] < self.tile_size or tile.shape[1] < self.tile_size:\n",
    "            padded_tile = np.zeros((self.tile_size, self.tile_size))\n",
    "            padded_tile[:tile.shape[0], :tile.shape[1]] = tile\n",
    "            return padded_tile\n",
    "            \n",
    "        return tile\n",
    "        \n",
    "    def read_row(self, tile_start: Tuple[int, int], row_idx: int, channel: int) -> np.ndarray:\n",
    "        \"\"\"Read one row from a tile\"\"\"\n",
    "        tile = self.read_tile(tile_start, channel)\n",
    "        if row_idx < tile.shape[0]:\n",
    "            return tile[row_idx]\n",
    "        return np.zeros(tile.shape[1])\n",
    "\n",
    "class ToeplitzBuffer:\n",
    "    \"\"\"Register buffer converting tiles to GEMM-ready activation vectors\"\"\"\n",
    "    def __init__(self, tile_size: int, kernel_size: int):\n",
    "        self.tile_size = tile_size\n",
    "        self.kernel_size = kernel_size\n",
    "        self.buffer = []  # Circular buffer holding K rows\n",
    "        self.output_width = tile_size - kernel_size + 1\n",
    "        \n",
    "    def stream_row(self, row_data: np.ndarray):\n",
    "        \"\"\"Stream one row into the buffer\"\"\"\n",
    "        # Add new row, remove old if buffer full\n",
    "        if len(self.buffer) >= self.kernel_size:\n",
    "            self.buffer.pop(0)\n",
    "        self.buffer.append(row_data.copy())\n",
    "        \n",
    "    def can_generate_vectors(self) -> bool:\n",
    "        \"\"\"Check if we have enough rows to generate activation vectors\"\"\"\n",
    "        return len(self.buffer) == self.kernel_size\n",
    "        \n",
    "    def generate_activation_vectors(self) -> List[List[float]]:\n",
    "        \"\"\"Generate flattened activation vectors for sliding windows\"\"\"\n",
    "        if not self.can_generate_vectors():\n",
    "            return []\n",
    "            \n",
    "        vectors = []\n",
    "        for col_start in range(self.output_width):\n",
    "            # Extract KxK patch and flatten\n",
    "            vector = []\n",
    "            for row_idx in range(self.kernel_size):\n",
    "                for col_idx in range(col_start, col_start + self.kernel_size):\n",
    "                    if col_idx < len(self.buffer[row_idx]):\n",
    "                        vector.append(float(self.buffer[row_idx][col_idx]))\n",
    "                    else:\n",
    "                        vector.append(0.0)\n",
    "            vectors.append(vector)\n",
    "            \n",
    "        return vectors\n",
    "\n",
    "class PsumBuffer:\n",
    "    \"\"\"Buffer for accumulating partial sums across tiles and channels\"\"\"\n",
    "    def __init__(self, output_height: int, output_width: int, num_kernels: int):\n",
    "        self.output_height = output_height\n",
    "        self.output_width = output_width\n",
    "        self.num_kernels = num_kernels\n",
    "        self.buffer = np.zeros((output_height, output_width, num_kernels))\n",
    "        \n",
    "    def accumulate(self, tile_output: np.ndarray, tile_start_out: Tuple[int, int], active_kernels: int):\n",
    "        \"\"\"Accumulate tile output into buffer\"\"\"\n",
    "        out_row_start, out_col_start = tile_start_out\n",
    "        tile_h, tile_w = tile_output.shape\n",
    "        \n",
    "        # Determine valid region\n",
    "        valid_h = min(tile_h, self.output_height - out_row_start)\n",
    "        valid_w = min(tile_w, self.output_width - out_col_start)\n",
    "        valid_k = min(active_kernels, self.num_kernels)\n",
    "        \n",
    "        if valid_h > 0 and valid_w > 0:\n",
    "            for k in range(valid_k):\n",
    "                self.buffer[out_row_start:out_row_start+valid_h, \n",
    "                           out_col_start:out_col_start+valid_w, k] += tile_output[:valid_h, :valid_w]\n",
    "                           \n",
    "    def get_final(self) -> np.ndarray:\n",
    "        \"\"\"Get final accumulated output\"\"\"\n",
    "        return self.buffer.copy()\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Reset buffer\"\"\"\n",
    "        self.buffer.fill(0.0)\n",
    "\n",
    "class ConvEngine:\n",
    "    \"\"\"Main convolution engine integrating all components\"\"\"\n",
    "    def __init__(self, input_tensor: np.ndarray, kernels: np.ndarray, \n",
    "                 tile_size: int = 16, stride: int = 1):\n",
    "        self.input_tensor = input_tensor\n",
    "        self.kernels = kernels\n",
    "        self.tile_size = tile_size\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.H, self.W, self.C = input_tensor.shape\n",
    "        self.K, self.R, self.S, self.C_k = kernels.shape\n",
    "        \n",
    "        assert self.C == self.C_k, f\"Channel mismatch: input {self.C}, kernels {self.C_k}\"\n",
    "        \n",
    "        # Calculate output dimensions\n",
    "        self.H_out = (self.H - self.R) // stride + 1\n",
    "        self.W_out = (self.W - self.R) // stride + 1\n",
    "        \n",
    "        # Initialize components\n",
    "        self.sa = SystolicArray()\n",
    "        self.kernel_loader = KernelLoader(kernels)\n",
    "        self.spm = ScratchpadMemory(input_tensor, tile_size, self.R, stride)\n",
    "        self.psum_buffer = PsumBuffer(self.H_out, self.W_out, self.K)\n",
    "        \n",
    "        print(f\"ConvEngine initialized:\")\n",
    "        print(f\"  Input: {self.H}x{self.W}x{self.C}\")\n",
    "        print(f\"  Kernels: {self.K} kernels of size {self.R}x{self.S}x{self.C_k}\")\n",
    "        print(f\"  Output: {self.H_out}x{self.W_out}x{self.K}\")\n",
    "        print(f\"  Tile size: {tile_size}\")\n",
    "        \n",
    "    def run(self) -> np.ndarray:\n",
    "        \"\"\"Run the convolution accelerator\"\"\"\n",
    "        print(\"\\nStarting convolution...\")\n",
    "        \n",
    "        # Process kernels in groups of 32 (SA width constraint)\n",
    "        for kernel_group_start in range(0, self.K, 32):\n",
    "            active_kernels = min(32, self.K - kernel_group_start)\n",
    "            print(f\"\\nProcessing kernel group {kernel_group_start//32 + 1}: kernels {kernel_group_start}-{kernel_group_start+active_kernels-1}\")\n",
    "            \n",
    "            # Load weights for this kernel group\n",
    "            weight_matrix = self.kernel_loader.get_kernel_matrix(kernel_group_start, active_kernels)\n",
    "            self.sa.load_weights(weight_matrix)\n",
    "            \n",
    "            # Process each channel\n",
    "            for channel in range(self.C):\n",
    "                print(f\"  Channel {channel + 1}/{self.C}\")\n",
    "                \n",
    "                # Get tile addresses\n",
    "                tile_addresses = self.spm.generate_tile_addresses()\n",
    "                \n",
    "                # Process each tile\n",
    "                for tile_idx, tile_start in enumerate(tile_addresses):\n",
    "                    # Reset SA accumulators for new tile\n",
    "                    self.sa.reset_accumulators()\n",
    "                    \n",
    "                    # Create Toeplitz buffer for this tile\n",
    "                    toeplitz_buffer = ToeplitzBuffer(self.tile_size, self.R)\n",
    "                    \n",
    "                    # Stream tile rows through Toeplitz buffer\n",
    "                    for row_idx in range(self.tile_size):\n",
    "                        row_data = self.spm.read_row(tile_start, row_idx, channel)\n",
    "                        toeplitz_buffer.stream_row(row_data)\n",
    "                        \n",
    "                        # Generate and process activation vectors when ready\n",
    "                        if toeplitz_buffer.can_generate_vectors():\n",
    "                            vectors = toeplitz_buffer.generate_activation_vectors()\n",
    "                            \n",
    "                            # Feed each vector through SA\n",
    "                            for vector in vectors:\n",
    "                                self.sa.feed_activation_row(vector)\n",
    "                                self.sa.cycle()\n",
    "                    \n",
    "                    # Collect PSUMs from SA\n",
    "                    psums = self.sa.collect_output()\n",
    "                    \n",
    "                    # Convert PSUMs to spatial tile output\n",
    "                    output_size = self.tile_size - self.R + 1\n",
    "                    if output_size > 0:\n",
    "                        # Reshape PSUMs into spatial output\n",
    "                        num_spatial_outputs = output_size * output_size\n",
    "                        \n",
    "                        # Create tile output (spatial_h, spatial_w) for each active kernel\n",
    "                        tile_output = np.zeros((output_size, output_size))\n",
    "                        \n",
    "                        # For simplicity, average the PSUMs over spatial positions\n",
    "                        # In real hardware, PSUMs correspond to specific spatial locations\n",
    "                        avg_psum = np.mean(psums[:active_kernels]) if active_kernels > 0 else 0.0\n",
    "                        tile_output.fill(avg_psum)\n",
    "                        \n",
    "                        # Calculate output tile position\n",
    "                        tile_row = tile_idx // self.spm.tile_grid_w\n",
    "                        tile_col = tile_idx % self.spm.tile_grid_w\n",
    "                        out_row_start = tile_row * self.spm.tile_step\n",
    "                        out_col_start = tile_col * self.spm.tile_step\n",
    "                        \n",
    "                        # Accumulate in PSUM buffer\n",
    "                        self.psum_buffer.accumulate(\n",
    "                            tile_output, \n",
    "                            (out_row_start, out_col_start), \n",
    "                            active_kernels\n",
    "                        )\n",
    "        \n",
    "        print(\"\\nConvolution completed!\")\n",
    "        return self.psum_buffer.get_final()\n",
    "\n",
    "def test_conv_engine():\n",
    "    \"\"\"Test the convolution engine with a simple example\"\"\"\n",
    "    print(\"Testing 32x32 Systolic Array Convolution Engine\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create test input\n",
    "    np.random.seed(42)\n",
    "    input_tensor = np.random.randn(64, 64, 4).astype(np.float32)\n",
    "    \n",
    "    # Create test kernels\n",
    "    kernels = np.random.randn(8, 3, 3, 4).astype(np.float32)\n",
    "    \n",
    "    # Create and run convolution engine\n",
    "    engine = ConvEngine(input_tensor, kernels, tile_size=8, stride=1)\n",
    "    \n",
    "    try:\n",
    "        output = engine.run()\n",
    "        \n",
    "        print(f\"\\nResults:\")\n",
    "        print(f\"  Output shape: {output.shape}\")\n",
    "        print(f\"  Output stats: mean={np.mean(output):.4f}, std={np.std(output):.4f}\")\n",
    "        print(f\"  Output range: [{np.min(output):.4f}, {np.max(output):.4f}]\")\n",
    "        \n",
    "        # Verify output dimensions\n",
    "        expected_h = (input_tensor.shape[0] - kernels.shape[1]) + 1\n",
    "        expected_w = (input_tensor.shape[1] - kernels.shape[2]) + 1\n",
    "        expected_k = kernels.shape[0]\n",
    "        \n",
    "        print(f\"\\nVerification:\")\n",
    "        print(f\"  Expected output shape: ({expected_h}, {expected_w}, {expected_k})\")\n",
    "        print(f\"  Actual output shape: {output.shape}\")\n",
    "        print(f\"  Shape match: {output.shape == (expected_h, expected_w, expected_k)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during convolution: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "def test_systolic_array():\n",
    "    \"\"\"Test individual systolic array functionality\"\"\"\n",
    "    print(\"\\nTesting Systolic Array Component\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    sa = SystolicArray()\n",
    "    \n",
    "    # Test weight loading\n",
    "    test_weights = np.random.randn(8, 4).astype(np.float32)  # 8x4 weight matrix\n",
    "    sa.load_weights(test_weights)\n",
    "    print(f\"Loaded {test_weights.shape} weight matrix\")\n",
    "    \n",
    "    # Test activation feeding and cycling\n",
    "    sa.reset_accumulators()\n",
    "    \n",
    "    for i in range(3):\n",
    "        test_activations = [1.0, 2.0, 3.0, 4.0] + [0.0] * 28  # Pad to 32\n",
    "        sa.feed_activation_row(test_activations)\n",
    "        sa.cycle()\n",
    "        print(f\"Cycle {i+1}: Fed activations\")\n",
    "    \n",
    "    # Collect output\n",
    "    psums = sa.collect_output()\n",
    "    print(f\"Collected {len(psums)} PSUMs\")\n",
    "    print(f\"First 4 PSUMs: {psums[:4]}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_systolic_array()\n",
    "    test_conv_engine()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
