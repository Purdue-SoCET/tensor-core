{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "# ==========================================================\n",
    "# 1. Scratchpad Memory (SPM) - handles tiling and padding\n",
    "# ==========================================================\n",
    "class ScratchpadMemory:\n",
    "    def __init__(self, input_tensor, tile_size, kernel_size, stride=1, verbose=True):\n",
    "        self.input = input_tensor\n",
    "        self.H, self.W, self.C = input_tensor.shape\n",
    "        self.tile_size = tile_size\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def generate_tile_addresses(self):\n",
    "        T, K, s = self.tile_size, self.kernel_size, self.stride\n",
    "        addrs = []\n",
    "        row_step = T - K + 1 if T - K + 1 > 0 else 1\n",
    "        col_step = T - K + 1 if T - K + 1 > 0 else 1\n",
    "\n",
    "        num_tiles_row = int(np.ceil((self.H - K + 1) / row_step))\n",
    "        num_tiles_col = int(np.ceil((self.W - K + 1) / col_step))\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"\\n[SPM] Generating tile addresses...\")\n",
    "            print(f\"  Input shape = ({self.H},{self.W},{self.C}), Tile size = {T}, Kernel = {K}, Stride = {s}\")\n",
    "            print(f\"  Step sizes -> row_step = {row_step}, col_step = {col_step}\")\n",
    "            print(f\"  Expected total tiles = {num_tiles_row} x {num_tiles_col} = {num_tiles_row*num_tiles_col}\\n\")\n",
    "\n",
    "        for t_i in range(num_tiles_row):\n",
    "            for t_j in range(num_tiles_col):\n",
    "                row = t_i * row_step\n",
    "                col = t_j * col_step\n",
    "                addrs.append((row, col))\n",
    "                if self.verbose:\n",
    "                    pad_r = max(0, row + T - self.H)\n",
    "                    pad_c = max(0, col + T - self.W)\n",
    "                    print(f\"  -> Tile ({t_i},{t_j}): start=({row},{col}), \"\n",
    "                          f\"cover=({row}:{row+T}, {col}:{col+T}), \"\n",
    "                          f\"pad_rows={pad_r}, pad_cols={pad_c}\")\n",
    "\n",
    "        return addrs\n",
    "\n",
    "    def read_row(self, tile_start, row_idx):\n",
    "        r0, c0 = tile_start\n",
    "        T = self.tile_size\n",
    "        row_data = np.zeros((T, self.C))\n",
    "        for j in range(T):\n",
    "            rr, cc = r0 + row_idx, c0 + j\n",
    "            if rr < self.H and cc < self.W:\n",
    "                row_data[j, :] = self.input[rr, cc, :]\n",
    "        # if self.verbose:\n",
    "        #     print(f\"[SPM] Read row {row_idx} from tile starting at {tile_start}, \"\n",
    "        #           f\"valid range=({r0+row_idx},{c0}:{c0+T})\")\n",
    "        return row_data  # shape [T, C]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToeplitzBuffer:\n",
    "    def __init__(self, tile_size, kernel_size, num_channels, stride=1):\n",
    "        self.tile_size = tile_size\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_channels = num_channels\n",
    "        self.stride = stride\n",
    "        self.buffer = []\n",
    "\n",
    "    def fill_buffer(self, row_data):\n",
    "        self.buffer.append(row_data)\n",
    "\n",
    "    def stream_columns(self):\n",
    "        \"\"\"Return list of column-major Toeplitz vectors for this tile.\"\"\"\n",
    "        K, T, C = self.kernel_size, self.tile_size, self.num_channels\n",
    "        if len(self.buffer) < K:\n",
    "            raise ValueError(\"Buffer not full yet\")\n",
    "\n",
    "        block = np.stack(self.buffer[-K:], axis=0)  # [K, T, C]\n",
    "        cols = []\n",
    "        for j in range(T - K + 1):\n",
    "            patch = block[:, j:j+K, :].reshape(-1)\n",
    "            cols.append(patch)\n",
    "        return cols\n",
    "\n",
    "    def stream_row(self, row_data):\n",
    "        \"\"\"\n",
    "        row_data: shape [tile_size, num_channels]\n",
    "        Returns: list of Toeplitz rows (flattened) ready for SA\n",
    "        \"\"\"\n",
    "        self.buffer.append(row_data)\n",
    "        if len(self.buffer) < self.kernel_size:\n",
    "            return []\n",
    "\n",
    "        K = self.kernel_size\n",
    "        C = self.num_channels\n",
    "        toeplitz_rows = []\n",
    "\n",
    "        current_block = np.stack(self.buffer[-K:], axis=0)  # shape [K, tile_size, C]\n",
    "\n",
    "        for j in range(0, self.tile_size - K + 1, self.stride):\n",
    "            patch = current_block[:, j:j+K, :].reshape(-1)\n",
    "            toeplitz_rows.append(patch)\n",
    "\n",
    "        return toeplitz_rows\n",
    "\n",
    "    # def stream_columns(self):\n",
    "    #     \"\"\"\n",
    "    #     Converts the buffered rows into column-major flattened vectors\n",
    "    #     ready for column-staggered SA streaming.\n",
    "    #     Returns: list of arrays of length K*num_channels (for SA row input)\n",
    "    #     \"\"\"\n",
    "    #     K = self.kernel_size\n",
    "    #     C = self.num_channels\n",
    "    #     T = self.tile_size\n",
    "\n",
    "    #     if len(self.buffer) < K:\n",
    "    #         return []\n",
    "\n",
    "    #     # Stack last K rows\n",
    "    #     block = np.stack(self.buffer[-K:], axis=0)  # [K, T, C]\n",
    "\n",
    "    #     cols = []\n",
    "    #     for j in range(0, T - K + 1, self.stride):\n",
    "    #         col_vec = block[:, j:j+K, :].reshape(-1)\n",
    "    #         # padded_col = np.zeros(self.tile_size)  # pad to array_size\n",
    "    #         # padded_col[:len(col_vec)] = col_vec\n",
    "    #         cols.append(col_vec)\n",
    "    #     return cols\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelLoader:\n",
    "    def __init__(self, kernels: np.ndarray, array_size: int):\n",
    "        self.R, self.S, self.C, self.K = kernels.shape\n",
    "        self.kernels = kernels\n",
    "        self.array_size = array_size\n",
    "\n",
    "    def get_kernel_matrix(self, k: int):\n",
    "        \"\"\"\n",
    "        Flatten one kernel and return padded [array_size, array_size] matrix.\n",
    "        \"\"\"\n",
    "        k_flat = self.kernels[:, :, :, k].reshape(-1)  # shape [R*S*C]\n",
    "        H = len(k_flat)\n",
    "        padded = np.zeros((self.array_size, self.array_size))\n",
    "        padded[:H, 0] = k_flat  # map into first column (common for weight-stationary)\n",
    "        return padded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PE:\n",
    "#     def __init__(self, links: Optional[List['PE']] = None):\n",
    "#         if links is None:\n",
    "#             links = [None, None, None, None]\n",
    "#         self.links = links\n",
    "#         self.activation = 0.0\n",
    "#         self.weight = 0.0\n",
    "#         self.accumulation = 0.0\n",
    "\n",
    "# class SystolicArray:\n",
    "#     def __init__(self, size: int):\n",
    "#         self.size = size\n",
    "#         self.array = [[PE() for _ in range(size)] for _ in range(size)]\n",
    "\n",
    "#     def load_weights(self, weight_matrix):\n",
    "#         \"\"\"Load a stationary weight matrix into the SA\"\"\"\n",
    "#         for i in range(self.size):\n",
    "#             for j in range(self.size):\n",
    "#                 self.array[i][j].weight = weight_matrix[i, j]\n",
    "\n",
    "#     def feed_activation_col(self, col_vec):\n",
    "#         \"\"\"\n",
    "#         Feed one Toeplitz column (vector) into the SA, staggered.\n",
    "#         col_vec: list/np.array of activations (length â‰¤ size).\n",
    "#         - Values injected into column 0, one per row, with delay.\n",
    "#         - Requires calling cycle() repeatedly to flush.\n",
    "#         \"\"\"\n",
    "#         depth = len(col_vec)\n",
    "#         # Stream with stagger (pipeline fill)\n",
    "#         for t in range(depth + self.size - 1):\n",
    "#             for r in range(self.size):\n",
    "#                 idx = t - r\n",
    "#                 if 0 <= idx < depth:\n",
    "#                     self.array[r][0].activation = col_vec[idx]\n",
    "#                 else:\n",
    "#                     self.array[r][0].activation = 0.0\n",
    "#             self.cycle()\n",
    "\n",
    "#     def cycle(self):\n",
    "#         \"\"\"One systolic cycle: MAC + shift activations rightward\"\"\"\n",
    "#         # 1. Compute MAC\n",
    "#         for i in range(self.size):\n",
    "#             for j in range(self.size):\n",
    "#                 pe = self.array[i][j]\n",
    "#                 pe.accumulation += pe.activation * pe.weight\n",
    "\n",
    "#         # 2. Shift activations right\n",
    "#         for i in range(self.size):\n",
    "#             for j in reversed(range(self.size - 1)):\n",
    "#                 self.array[i][j + 1].activation = self.array[i][j].activation\n",
    "\n",
    "#     def collect_output(self):\n",
    "#         out = np.zeros((self.size, self.size))\n",
    "#         for i in range(self.size):\n",
    "#             for j in range(self.size):\n",
    "#                 out[i, j] = self.array[i][j].accumulation\n",
    "#         return out\n",
    "\n",
    "#     def reset_accumulation(self):\n",
    "#         for i in range(self.size):\n",
    "#             for j in range(self.size):\n",
    "#                 self.array[i][j].accumulation = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PE:\n",
    "#     def __init__(self, links: Optional[List['PE']] = None):\n",
    "#         if links is None:\n",
    "#             links = [None, None, None, None]\n",
    "#         self.links = links\n",
    "#         self.activation = 0.0\n",
    "#         self.weight = 0.0\n",
    "#         self.accumulation = 0.0\n",
    "\n",
    "# class SystolicArray:\n",
    "#     def __init__(self, size: int):\n",
    "#         self.size = size\n",
    "#         self.array = [[PE() for _ in range(size)] for _ in range(size)]\n",
    "\n",
    "#     def load_weights(self, weight_matrix):\n",
    "#         for i in range(self.size):\n",
    "#             for j in range(self.size):\n",
    "#                 self.array[i][j].weight = weight_matrix[i,j]\n",
    "\n",
    "#     def feed_activation_row(self, row):\n",
    "#         for j in range(len(row)):\n",
    "#             self.array[0][j].activation = row[j]\n",
    "\n",
    "#     def cycle(self):\n",
    "#         # compute accumulation\n",
    "#         for i in range(self.size):\n",
    "#             for j in range(self.size):\n",
    "#                 pe = self.array[i][j]\n",
    "#                 pe.accumulation += pe.activation * pe.weight\n",
    "#         # shift activations down\n",
    "#         for i in reversed(range(self.size-1)):\n",
    "#             for j in range(self.size):\n",
    "#                 self.array[i+1][j].activation = self.array[i][j].activation\n",
    "\n",
    "#     def collect_output(self):\n",
    "#         out = np.zeros((self.size, self.size))\n",
    "#         for i in range(self.size):\n",
    "#             for j in range(self.size):\n",
    "#                 out[i,j] = self.array[i][j].accumulation\n",
    "#         return out\n",
    "\n",
    "#     def reset_accumulation(self):\n",
    "#         for i in range(self.size):\n",
    "#             for j in range(self.size):\n",
    "#                 self.array[i][j].accumulation = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "    \n",
    "class SystolicArray():\n",
    "    def __init__(self, size : int, PE_latency : int, dtype : np.dtype):\n",
    "        self.size = size\n",
    "        self.PE_latency = PE_latency\n",
    "        self.dtype = dtype\n",
    "        self.weight            = np.zeros(shape=(size, size), dtype=dtype)\n",
    "        self.PE_total          = np.zeros(shape=(size, size), dtype=dtype)\n",
    "        self.input_from_left   = np.zeros(shape=(size, size), dtype=dtype)\n",
    "        self.total_from_above  = np.zeros(shape=(size, size), dtype=dtype)\n",
    "        self.output_right      = np.zeros(shape=(size, size), dtype=dtype)\n",
    "        self.output_down       = np.zeros(shape=(size, size), dtype=dtype)\n",
    "        self.partial_sum_FIFOs = np.zeros(shape=(2 * size, size), dtype=dtype)\n",
    "        self.input_FIFOs       = np.zeros(shape=(size, 2 * size), dtype=dtype)\n",
    "        self.output_FIFOs      = np.zeros(shape=(2 * size, size), dtype=dtype)\n",
    "\n",
    "    def load_weights(self, weight):\n",
    "        self.weight = weight.T\n",
    "    \n",
    "    def load_input_FIFOs(self, input):\n",
    "        for i in range(self.size):\n",
    "            self.input_FIFOs[i, self.size - i: 2 * self.size - i] = input[i, :]\n",
    "\n",
    "    def load_partial_sum_FIFOs(self, PS):\n",
    "        for i in range(self.size):\n",
    "            self.partial_sum_FIFOs[self.size - i : 2 * self.size - i, i] = PS[i, :]\n",
    "        \n",
    "    def read_output_FIFOs(self):\n",
    "        out = np.zeros(shape = (self.size, self.size), dtype = self.dtype)\n",
    "        for i in range(self.size):\n",
    "            out[i, :] = self.output_FIFOs[self.size - i : 2 * self.size - i, i]\n",
    "        return out\n",
    "\n",
    "\n",
    "    def cycle(self):\n",
    "        # Write to output FIFOs\n",
    "        self.output_FIFOs = np.roll(self.output_FIFOs, 1, axis = 0)\n",
    "        self.output_FIFOs[0, :] = self.output_down[-1, :]\n",
    "\n",
    "        # Move outputs down\n",
    "        self.total_from_above = np.roll(self.output_down, 1, axis = 0)\n",
    "\n",
    "        # Read partial sum into top row\n",
    "        self.total_from_above[0,:] = self.partial_sum_FIFOs[-1, :]\n",
    "        self.partial_sum_FIFOs = np.roll(self.partial_sum_FIFOs, 1, axis = 0)\n",
    "        self.partial_sum_FIFOs[0, :] = self.dtype(0)\n",
    "\n",
    "        # Move inputs right\n",
    "        self.input_from_left = np.roll(self.output_right, 1, axis = 1)\n",
    "\n",
    "        # Write new input into left column\n",
    "        self.input_from_left[:, 0] = self.input_FIFOs[:, -1]\n",
    "        self.input_FIFOs = np.roll(self.input_FIFOs, 1, axis = 1)\n",
    "        self.input_FIFOs[:, 0] = self.dtype(0)\n",
    "        \n",
    "        #Perform MAC computations\n",
    "        self.PE_total = (self.weight * self.input_from_left).astype(self.dtype) + self.total_from_above\n",
    "        self.output_down = self.PE_total\n",
    "        self.output_right = self.input_from_left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# 6. Accumulator\n",
    "# ==========================================================\n",
    "class PsumBuffer:\n",
    "    def __init__(self):\n",
    "        self.outputs = []\n",
    "\n",
    "    def accumulate(self, tile_output):\n",
    "        self.outputs.append(tile_output)\n",
    "\n",
    "    def get_final(self):\n",
    "        return np.sum(self.outputs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvEngine:\n",
    "    def __init__(self, input_tensor, kernels, array_size=32, stride=1, verbose=True):\n",
    "        self.input_tensor = input_tensor\n",
    "        self.kernels = kernels\n",
    "        self.array_size = array_size\n",
    "        self.stride = stride\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.H, self.W, self.C = input_tensor.shape\n",
    "        self.R, self.S, self.Ck, self.K = kernels.shape\n",
    "        assert self.C == self.Ck, \"Channel mismatch!\"\n",
    "\n",
    "        self.sa = SystolicArray(array_size)\n",
    "\n",
    "    # # --- Diagonal wavefront streaming into SA ---\n",
    "    # def stream_to_sa(sa, toeplitz_cols, array_size):\n",
    "    #     \"\"\"\n",
    "    #     sa: SystolicArray object\n",
    "    #     toeplitz_cols: list of flattened activation vectors (length <= array_size)\n",
    "    #     \"\"\"\n",
    "    #     num_cycles = len(toeplitz_cols) + array_size - 1\n",
    "    #     for t in range(num_cycles):\n",
    "    #         for col_idx in range(array_size):\n",
    "    #             if 0 <= t - col_idx < len(toeplitz_cols):\n",
    "    #                 act_vec = toeplitz_cols[t - col_idx]\n",
    "    #             else:\n",
    "    #                 act_vec = np.zeros(array_size)\n",
    "\n",
    "    #             # pad vector to array_size\n",
    "    #             if len(act_vec) < array_size:\n",
    "    #                 padded = np.zeros(array_size)\n",
    "    #                 padded[:len(act_vec)] = act_vec\n",
    "    #                 act_vec = padded\n",
    "\n",
    "    #             # feed activations into SA\n",
    "    #             for r in range(array_size):\n",
    "    #                 sa.array[r][col_idx].activation = act_vec[r]\n",
    "\n",
    "    #         sa.cycle()\n",
    "        \n",
    "        # --- Main SA simulation for a tile ---\n",
    "    # def process_tile(sa, spm, tile_start, kernels, array_size):\n",
    "    #     \"\"\"\n",
    "    #     sa: SystolicArray object\n",
    "    #     spm: ScratchpadMemory object\n",
    "    #     tile_start: (row, col)\n",
    "    #     kernels: R x S x C x K\n",
    "    #     \"\"\"\n",
    "    #     R, S, C, K = kernels.shape\n",
    "    #     toeplitz = ToeplitzBuffer(array_size, R, C)\n",
    "    #     # Fill buffer with tile rows\n",
    "    #     for r in range(array_size):\n",
    "    #         row_data = spm.read_row(tile_start, r)\n",
    "    #         toeplitz.fill_buffer(row_data)\n",
    "\n",
    "    #     toeplitz_cols = toeplitz.stream_columns()\n",
    "\n",
    "    #     # For each kernel\n",
    "    #     tile_psum = np.zeros((len(toeplitz_cols), K))\n",
    "    #     for k_idx in range(K):\n",
    "    #         weight_flat = kernels[:, :, :, k_idx].reshape(-1)\n",
    "    #         # pad weights to array_size\n",
    "    #         weight_chunk = np.zeros(array_size)\n",
    "    #         weight_chunk[:len(weight_flat)] = weight_flat\n",
    "    #         weight_matrix = np.zeros((array_size, array_size))\n",
    "    #         weight_matrix[:, 0] = weight_chunk  # weight-stationary\n",
    "    #         sa.load_weights(weight_matrix)\n",
    "\n",
    "    #         # Stream activations\n",
    "    #         stream_to_sa(sa, toeplitz_cols, array_size)\n",
    "\n",
    "    #         # Collect SA output for this kernel\n",
    "    #         sa_out = sa.collect_output()\n",
    "    #         tile_psum[:, k_idx] = sa_out[:len(toeplitz_cols), 0]\n",
    "\n",
    "    #         sa.reset_accumulation()\n",
    "\n",
    "    #     return tile_psum\n",
    "        \n",
    "    def run(self):\n",
    "        spm = ScratchpadMemory(self.input_tensor, self.array_size, self.R,\n",
    "                            stride=self.stride, verbose=self.verbose)\n",
    "\n",
    "        # Output dimensions\n",
    "        out_H = (self.H - self.R) // self.stride + 1\n",
    "        out_W = (self.W - self.S) // self.stride + 1\n",
    "        output = np.zeros((out_H, out_W, self.K))\n",
    "\n",
    "        # 1. Get all tile start addresses\n",
    "        tile_addrs = spm.generate_tile_addresses()\n",
    "        if self.verbose:\n",
    "            print(f\"\\n[Engine] === Tiling complete: {len(tile_addrs)} tiles ===\")\n",
    "\n",
    "        # 2. Process kernels in groups of SA width\n",
    "        group_size = self.array_size\n",
    "        num_groups = int(np.ceil(self.K / group_size))\n",
    "        for g in range(num_groups):\n",
    "            k_start = g * group_size\n",
    "            k_end = min((g+1) * group_size, self.K)\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f\"\\n[Engine] === Processing kernel group {g} ({k_start}..{k_end-1}) ===\")\n",
    "                print()\n",
    "\n",
    "            # Reset SA accumulators\n",
    "            self.sa.reset_accumulation()\n",
    "\n",
    "            # Figure out how many chunks are needed (same across all kernels in this group)\n",
    "            max_chunks = max(\n",
    "                int(np.ceil(self.kernels[:, :, :, k].size / self.array_size))\n",
    "                for k in range(k_start, k_end)\n",
    "            )\n",
    "\n",
    "            # ðŸ”‘ Loop over chunks round-robin style\n",
    "            for chunk_id in range(max_chunks):\n",
    "                # 1. Load this chunk for ALL kernels in the group\n",
    "                weight_matrix = np.zeros((self.array_size, self.array_size))\n",
    "\n",
    "                for col, k in enumerate(range(k_start, k_end)):\n",
    "                    k_flat = self.kernels[:, :, :, k].reshape(-1)\n",
    "                    start = chunk_id * self.array_size\n",
    "                    end = min((chunk_id + 1) * self.array_size, len(k_flat))\n",
    "\n",
    "                    chunk = np.zeros(self.array_size)\n",
    "                    if start < len(k_flat):\n",
    "                        chunk[:end - start] = k_flat[start:end]\n",
    "\n",
    "                    weight_matrix[:, col] = chunk\n",
    "\n",
    "                    if self.verbose:\n",
    "                        print(f\"[Kernel {k}] Chunk {chunk_id+1}/{max_chunks} loaded \"\n",
    "                            f\"indices {start}:{end} into column {col}\")\n",
    "                        \n",
    "                # if self.verbose:\n",
    "                #     print(f\"\\nWeight matrix for chunk {chunk_id+1}:\\n{weight_matrix}\\n\")\n",
    "\n",
    "                # for c in range(weight_matrix.shape[1]):\n",
    "                #     print(f\"Column {c}: {weight_matrix[:, c]}\")\n",
    "\n",
    "                self.sa.load_weights(weight_matrix)\n",
    "                self.sa.cycle()  # settle weights\n",
    "\n",
    "                # 2. Process all tiles with current chunk weights\n",
    "                for (row, col_start) in tile_addrs:\n",
    "                    if self.verbose:\n",
    "                        print(f\"\\n[Engine] Processing tile at start=({row},{col_start}) \"\n",
    "                            f\"for chunk {chunk_id+1}\")\n",
    "\n",
    "                    # toeplitz = ToeplitzBuffer(self.array_size, self.R, self.C,\n",
    "                    #                         stride=self.stride)\n",
    "\n",
    "                    # for r in range(self.array_size):\n",
    "                    #     row_data = spm.read_row((row, col_start), r)\n",
    "                    #     toeplitz_rows = toeplitz.stream_row(row_data)\n",
    "\n",
    "                    #     for act_row in toeplitz_rows:\n",
    "                    #         num_act_chunks = int(np.ceil(len(act_row) / self.array_size))\n",
    "                    #         for act_chunk_id in range(num_act_chunks):\n",
    "                    #             a_start = act_chunk_id * self.array_size\n",
    "                    #             a_end = min((act_chunk_id + 1) * self.array_size, len(act_row))\n",
    "                    #             act_chunk = np.zeros(self.array_size)\n",
    "                    #             act_chunk[:a_end - a_start] = act_row[a_start:a_end]\n",
    "\n",
    "                    #             for r in range(self.array_size):\n",
    "                    #                 if r < len(act_chunk):\n",
    "                    #                     self.sa.array[r][0].activation = act_chunk[r]\n",
    "                    #                 else:\n",
    "                    #                     self.sa.array[r][0].activation = 0.0\n",
    "                    #             self.sa.cycle()\n",
    "                    toeplitz = ToeplitzBuffer(self.array_size, self.R, self.C, stride=self.stride)\n",
    "\n",
    "                    for r_idx in range(self.array_size):\n",
    "                        row_data = spm.read_row((row, col_start), r_idx)\n",
    "                        toeplitz.buffer.append(row_data)  # Fill the buffer for K rows\n",
    "\n",
    "                    # Convert buffered rows into column-major vectors\n",
    "                    toeplitz_cols = toeplitz.stream_columns()\n",
    "\n",
    "                    # Column-staggered streaming (diagonal wavefront)\n",
    "                    for t in range(len(toeplitz_cols) + self.array_size - 1):\n",
    "                        for col_idx in range(self.array_size):\n",
    "                            if 0 <= t - col_idx < len(toeplitz_cols):\n",
    "                                act_vec = toeplitz_cols[t - col_idx]\n",
    "                            else:\n",
    "                                act_vec = np.zeros(self.array_size)\n",
    "\n",
    "                            # Pad act_vec if it's shorter than array_size\n",
    "                            if len(act_vec) < self.array_size:\n",
    "                                padded_vec = np.zeros(self.array_size)\n",
    "                                padded_vec[:len(act_vec)] = act_vec\n",
    "                                act_vec = padded_vec\n",
    "                            for r in range(self.array_size):\n",
    "                                self.sa.array[r][col_idx].activation = act_vec[r]\n",
    "\n",
    "                        self.sa.cycle()\n",
    "                        print(f\"\\nCycle {t+1}:\")\n",
    "                        for col_idx in range(self.array_size):\n",
    "                            col_acts = [self.sa.array[r][col_idx].activation for r in range(self.array_size)]\n",
    "                            print(f\"Column {col_idx}: {col_acts}\")\n",
    "                    # for r in range(self.array_size):\n",
    "                    #     row_data = spm.read_row((row, col_start), r)\n",
    "                    #     toeplitz_cols = toeplitz.stream_row(row_data)\n",
    "\n",
    "                    #     # Each column is a Toeplitz vector â†’ feed directly\n",
    "                    #     for col_vec in toeplitz_cols:\n",
    "                    #         self.sa.feed_activation_col(col_vec)\n",
    "\n",
    "                    sa_out = self.sa.collect_output()  # final psums for tile-region\n",
    "\n",
    "                    # Write into output\n",
    "                    block_H = min(self.array_size, out_H - row)\n",
    "                    block_W = min(self.array_size, out_W - col_start)\n",
    "                    for col_idx, k in enumerate(range(k_start, k_end)):\n",
    "                        valid_out = sa_out[:block_H, col_idx:col_idx + block_W]\n",
    "                        target_slice = output[row:row + block_H, col_start:col_start + block_W, k]\n",
    "                        min_H = min(valid_out.shape[0], target_slice.shape[0])\n",
    "                        min_W = min(valid_out.shape[1], target_slice.shape[1])\n",
    "                        target_slice[:min_H, :min_W] = valid_out[:min_H, :min_W]\n",
    "\n",
    "                    # Reset SA accumulations before next tile\n",
    "                    self.sa.reset_accumulation()\n",
    "                    toeplitz.buffer = []\n",
    "\n",
    "        return output\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvEngineMatmul:\n",
    "    def __init__(self, input_tensor, kernels, tile_size=32, stride=1, verbose=True):\n",
    "        \"\"\"\n",
    "        input_tensor: H x W x C\n",
    "        kernels: R x S x C x K\n",
    "        tile_size: for tiling (matches ScratchpadMemory.tile_size)\n",
    "        \"\"\"\n",
    "        self.input_tensor = input_tensor\n",
    "        self.kernels = kernels\n",
    "        self.tile_size = tile_size\n",
    "        self.stride = stride\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.H, self.W, self.C = input_tensor.shape\n",
    "        self.R, self.S, self.Ck, self.K = kernels.shape\n",
    "        assert self.C == self.Ck, \"Channel mismatch!\"\n",
    "\n",
    "        self.spm = ScratchpadMemory(input_tensor, tile_size, self.R,\n",
    "                                    stride=stride, verbose=verbose)\n",
    "\n",
    "    # def run(self):\n",
    "    #     # Output dimensions\n",
    "    #     out_H = (self.H - self.R) // self.stride + 1\n",
    "    #     out_W = (self.W - self.S) // self.stride + 1\n",
    "    #     output = np.zeros((out_H, out_W, self.K))\n",
    "\n",
    "    #     # 1. Generate tile addresses\n",
    "    #     tile_addrs = self.spm.generate_tile_addresses()\n",
    "    #     if self.verbose:\n",
    "    #         print(f\"\\n[Engine] Total tiles: {len(tile_addrs)}\")\n",
    "\n",
    "    #     # 2. Process each tile\n",
    "    #     for tile_idx, (row, col) in enumerate(tile_addrs):\n",
    "    #         if self.verbose:\n",
    "    #             print(f\"\\n[Tile {tile_idx}] Start at ({row},{col})\")\n",
    "\n",
    "    #         # Determine tile dimensions\n",
    "    #         tile_H = min(self.tile_size, out_H - row)\n",
    "    #         tile_W = min(self.tile_size, out_W - col)\n",
    "    #         num_patches = tile_H * tile_W\n",
    "\n",
    "    #         # Build Toeplitz-like patches for the tile\n",
    "    #         Tmat = np.zeros((num_patches, self.R * self.S * self.C))\n",
    "    #         patch_idx = 0\n",
    "    #         for i in range(row, row + tile_H * self.stride, self.stride):\n",
    "    #             for j in range(col, col + tile_W * self.stride, self.stride):\n",
    "    #                 patch = np.zeros((self.R, self.S, self.C))\n",
    "    #                 for r in range(self.R):\n",
    "    #                     row_data = self.spm.read_row((i, j), r)  # shape = [tile_size, C]\n",
    "    #                     # only take first S elements from row_data for this patch\n",
    "    #                     patch[r, :, :] = row_data[:self.S, :]\n",
    "    #                 Tmat[patch_idx, :] = patch.reshape(-1)\n",
    "        #             patch_idx += 1\n",
    "\n",
    "        #     if self.verbose:\n",
    "        #         print(f\"  Tmat shape: {Tmat.shape}\")\n",
    "\n",
    "        #     # 3. Multiply with all kernels\n",
    "        #     kernel_flat_list = [self.kernels[:, :, :, k].reshape(-1) for k in range(self.K)]\n",
    "        #     kernel_matrix = np.stack(kernel_flat_list, axis=1)  # shape = (R*S*C, K)\n",
    "        #     Y = Tmat @ kernel_matrix  # shape = (num_patches, K)\n",
    "\n",
    "        #     # 4. Write back to output\n",
    "        #     idx = 0\n",
    "        #     for i in range(tile_H):\n",
    "        #         for j in range(tile_W):\n",
    "        #             for k in range(self.K):\n",
    "        #                 output[row + i, col + j, k] = Y[idx, k]\n",
    "        #             idx += 1\n",
    "\n",
    "        # return output\n",
    "\n",
    "    def run(self):\n",
    "        # Output dimensions\n",
    "        out_H = (self.H - self.R) // self.stride + 1\n",
    "        out_W = (self.W - self.S) // self.stride + 1\n",
    "        output = np.zeros((out_H, out_W, self.K))\n",
    "\n",
    "        # 1. Generate tile addresses\n",
    "        tile_addrs = self.spm.generate_tile_addresses()\n",
    "        if self.verbose:\n",
    "            print(f\"\\n[Engine] Total tiles: {len(tile_addrs)}\")\n",
    "\n",
    "        # 2. Process each tile\n",
    "        for tile_idx, (row, col) in enumerate(tile_addrs):\n",
    "            if self.verbose:\n",
    "                print(f\"\\n[Tile {tile_idx}] Start at ({row},{col})\")\n",
    "\n",
    "            # Determine tile dimensions\n",
    "            # tile_H = min(self.tile_size, out_H - row)\n",
    "            # tile_W = min(self.tile_size, out_W - col)\n",
    "            tile_H = max(0, min(self.tile_size, out_H - row))\n",
    "            tile_W = max(0, min(self.tile_size, out_W - col))\n",
    "            if tile_H == 0 or tile_W == 0:\n",
    "                continue  # skip invalid tiles  \n",
    "            num_patches = tile_H * tile_W\n",
    "\n",
    "            # Build Toeplitz-like patches for the tile\n",
    "            Tmat = np.zeros((num_patches, self.R * self.S * self.C))\n",
    "            patch_idx = 0\n",
    "            # for i in range(row, row + tile_H * self.stride, self.stride):\n",
    "            #     for j in range(col, col + tile_W * self.stride, self.stride):\n",
    "            #         patch = np.zeros((self.R, self.S, self.C))\n",
    "            #         for r in range(self.R):\n",
    "            #             row_data = self.spm.read_row((i, j), r)  # shape = [tile_size, C]\n",
    "            #             patch[r, :, :] = row_data[:self.S, :]\n",
    "            #         Tmat[patch_idx, :] = patch.reshape(-1)\n",
    "            #         patch_idx += 1\n",
    "\n",
    "            for i_out in range(row, row + tile_H):       # output-space row\n",
    "                for j_out in range(col, col + tile_W):   # output-space col\n",
    "                    i_in = i_out * self.stride           # map to input-space row\n",
    "                    j_in = j_out * self.stride           # map to input-space col\n",
    "\n",
    "                    patch = np.zeros((self.R, self.S, self.C))\n",
    "                    for r in range(self.R):\n",
    "                        row_data = self.spm.read_row((i_in, j_in), r)  # shape = [tile_size, C]\n",
    "                        patch[r, :, :] = row_data[:self.S, :]\n",
    "                    Tmat[patch_idx, :] = patch.reshape(-1)\n",
    "                    patch_idx += 1\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f\"  Tmat shape: {Tmat.shape}\")\n",
    "\n",
    "            # 3. Multiply with kernels (chunked systolic style)\n",
    "            kernel_flat_list = [self.kernels[:, :, :, k].reshape(-1) for k in range(self.K)]\n",
    "            kernel_matrix = np.stack(kernel_flat_list, axis=1)  # shape = (R*S*C, K)\n",
    "\n",
    "            Y = np.zeros((num_patches, self.K))\n",
    "\n",
    "            # chunk Toeplitz and kernel matrix into blocks of at most 32\n",
    "            m, n = Tmat.shape  # m = num_patches, n = R*S*C\n",
    "            k = self.K\n",
    "\n",
    "            block_m = 32\n",
    "            block_k = 32\n",
    "\n",
    "            for i in range(0, m, block_m):   # patches dimension\n",
    "                for j in range(0, k, block_k):  # kernel dimension\n",
    "                    for p in range(0, n, block_k):  # inner dimension (reduction)\n",
    "                        T_block = Tmat[i:i+block_m, p:p+block_k]\n",
    "                        K_block = kernel_matrix[p:p+block_k, j:j+block_k]\n",
    "                        Y[i:i+block_m, j:j+block_k] += T_block @ K_block\n",
    "\n",
    "            # 4. Write back to output\n",
    "            idx = 0\n",
    "            for i in range(tile_H):\n",
    "                for j in range(tile_W):\n",
    "                    for k in range(self.K):\n",
    "                        output[row + i, col + j, k] = Y[idx, k]\n",
    "                    idx += 1\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvEngineSA:\n",
    "    def __init__(self, input_tensor, kernels, tile_size=32, stride=1, verbose=True, dtype=np.float16):\n",
    "        \"\"\"\n",
    "        input_tensor: H x W x C\n",
    "        kernels: R x S x C x K\n",
    "        \"\"\"\n",
    "        self.input_tensor = input_tensor\n",
    "        self.kernels = kernels\n",
    "        self.tile_size = tile_size\n",
    "        self.stride = stride\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.H, self.W, self.C = input_tensor.shape\n",
    "        self.R, self.S, self.Ck, self.K = kernels.shape\n",
    "        assert self.C == self.Ck, \"Channel mismatch!\"\n",
    "\n",
    "        self.spm = ScratchpadMemory(input_tensor, tile_size, self.R,\n",
    "                                    stride=stride, verbose=verbose)\n",
    "\n",
    "        # systolic array of size = tile_size (like before)\n",
    "        self.sa = SystolicArray(size=tile_size, PE_latency=1, dtype=dtype)\n",
    "\n",
    "    def run(self):\n",
    "        # Output dimensions\n",
    "        out_H = (self.H - self.R) // self.stride + 1\n",
    "        out_W = (self.W - self.S) // self.stride + 1\n",
    "        output = np.zeros((out_H, out_W, self.K), dtype=self.sa.dtype)\n",
    "\n",
    "        # Generate tiles\n",
    "        tile_addrs = self.spm.generate_tile_addresses()\n",
    "        if self.verbose:\n",
    "            print(f\"\\n[EngineSA] Total tiles: {len(tile_addrs)}\")\n",
    "\n",
    "        # --- Flatten all kernels into weight matrix (one kernel per column)\n",
    "        kernel_flat_list = [self.kernels[:, :, :, k].reshape(-1) for k in range(self.K)]\n",
    "        kernel_matrix = np.stack(kernel_flat_list, axis=1)  # (R*S*C, K)\n",
    "\n",
    "        # Pad to SA size (tile_size)\n",
    "        pad_len = self.tile_size * self.tile_size - kernel_matrix.shape[0]\n",
    "        if pad_len > 0:\n",
    "            kernel_matrix = np.vstack([kernel_matrix, np.zeros((pad_len, self.K), dtype=self.sa.dtype)])\n",
    "\n",
    "        # Load weights into systolic array (each kernel = one column)\n",
    "        W = np.zeros((self.sa.size, self.sa.size), dtype=self.sa.dtype)\n",
    "        for k in range(self.K):\n",
    "            W[:, k] = kernel_matrix[:self.sa.size, k]\n",
    "        self.sa.load_weights(W)\n",
    "\n",
    "        # Process each tile\n",
    "        for tile_idx, (row, col) in enumerate(tile_addrs):\n",
    "            if self.verbose:\n",
    "                print(f\"\\n[Tile {tile_idx}] Start at ({row},{col})\")\n",
    "\n",
    "            # tile_H = min(self.tile_size, out_H - row)\n",
    "            # tile_W = min(self.tile_size, out_W - col)\n",
    "            tile_H = max(0, min(self.tile_size, out_H - row))\n",
    "            tile_W = max(0, min(self.tile_size, out_W - col))\n",
    "\n",
    "            # --- Build Toeplitz-like patches\n",
    "            patches = []\n",
    "            for i in range(row, row + tile_H * self.stride, self.stride):\n",
    "                for j in range(col, col + tile_W * self.stride, self.stride):\n",
    "                    patch = np.zeros((self.R, self.S, self.C))\n",
    "                    for r in range(self.R):\n",
    "                        row_data = self.spm.read_row((i, j), r)  # shape [T, C]\n",
    "                        patch[r, :, :] = row_data[:self.S, :]\n",
    "                    patches.append(patch.reshape(-1))\n",
    "            Tmat = np.stack(patches, axis=0)  # (num_patches, R*S*C)\n",
    "\n",
    "            # Pad to SA size\n",
    "            num_patches = Tmat.shape[0]\n",
    "            if Tmat.shape[1] < self.sa.size:\n",
    "                pad = np.zeros((num_patches, self.sa.size - Tmat.shape[1]))\n",
    "                Tmat = np.hstack([Tmat, pad])\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"  -> Streaming {num_patches} patches into SA\")\n",
    "\n",
    "            # --- Stream into systolic array (row by row, diagonal wavefront)\n",
    "            for patch in Tmat:\n",
    "                act_matrix = np.zeros((self.sa.size, self.sa.size), dtype=self.sa.dtype)\n",
    "                act_matrix[:, 0] = patch[:self.sa.size]\n",
    "                self.sa.load_input_FIFOs(act_matrix)\n",
    "                self.sa.cycle()\n",
    "\n",
    "            # Flush pipeline (extra cycles to propagate)\n",
    "            for _ in range(2*self.sa.size):\n",
    "                self.sa.cycle()\n",
    "\n",
    "            # --- Collect outputs\n",
    "            sa_out = self.sa.read_output_FIFOs()  # (size, size)\n",
    "            if self.verbose:\n",
    "                print(f\"  SA output shape: {sa_out.shape}\")\n",
    "\n",
    "            # Write back (just top-left block relevant)\n",
    "            for i in range(tile_H):\n",
    "                for j in range(tile_W):\n",
    "                    for k in range(self.K):\n",
    "                        output[row+i, col+j, k] = sa_out[i, k]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SPM] Generating tile addresses...\n",
      "  Input shape = (64,64,4), Tile size = 32, Kernel = 5, Stride = 3\n",
      "  Step sizes -> row_step = 28, col_step = 28\n",
      "  Expected total tiles = 3 x 3 = 9\n",
      "\n",
      "  -> Tile (0,0): start=(0,0), cover=(0:32, 0:32), pad_rows=0, pad_cols=0\n",
      "  -> Tile (0,1): start=(0,28), cover=(0:32, 28:60), pad_rows=0, pad_cols=0\n",
      "  -> Tile (0,2): start=(0,56), cover=(0:32, 56:88), pad_rows=0, pad_cols=24\n",
      "  -> Tile (1,0): start=(28,0), cover=(28:60, 0:32), pad_rows=0, pad_cols=0\n",
      "  -> Tile (1,1): start=(28,28), cover=(28:60, 28:60), pad_rows=0, pad_cols=0\n",
      "  -> Tile (1,2): start=(28,56), cover=(28:60, 56:88), pad_rows=0, pad_cols=24\n",
      "  -> Tile (2,0): start=(56,0), cover=(56:88, 0:32), pad_rows=24, pad_cols=0\n",
      "  -> Tile (2,1): start=(56,28), cover=(56:88, 28:60), pad_rows=24, pad_cols=0\n",
      "  -> Tile (2,2): start=(56,56), cover=(56:88, 56:88), pad_rows=24, pad_cols=24\n",
      "\n",
      "[Engine] Total tiles: 9\n",
      "\n",
      "[Tile 0] Start at (0,0)\n",
      "  Tmat shape: (400, 100)\n",
      "\n",
      "[Tile 1] Start at (0,28)\n",
      "\n",
      "[Tile 2] Start at (0,56)\n",
      "\n",
      "[Tile 3] Start at (28,0)\n",
      "\n",
      "[Tile 4] Start at (28,28)\n",
      "\n",
      "[Tile 5] Start at (28,56)\n",
      "\n",
      "[Tile 6] Start at (56,0)\n",
      "\n",
      "[Tile 7] Start at (56,28)\n",
      "\n",
      "[Tile 8] Start at (56,56)\n",
      "Matmul Simulator Output Shape: (20, 20, 2)\n",
      "Golden Conv Shape: (20, 20, 2)\n",
      "Kernel 0 max absolute difference: 0.0\n",
      "Kernel 1 max absolute difference: 0.0\n",
      "\n",
      "Sample outputs (kernel 0, first few positions):\n",
      "SA output: [[379. 438. 401.]\n",
      " [369. 394. 439.]\n",
      " [431. 364. 425.]]\n",
      "Golden:    [[379. 438. 401.]\n",
      " [369. 394. 439.]\n",
      " [431. 364. 425.]]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    H, W, C = 64, 64, 4\n",
    "    R = S = 5\n",
    "    stride = 3\n",
    "    input_tensor = np.random.randint(0,5,(H,W,C)).astype(float)\n",
    "    kernels = np.random.randint(0,5,(R,S,C,2)).astype(float)\n",
    "\n",
    "    # engine1 = ConvEngine(input_tensor, kernels, stride=stride, array_size=32, verbose=True)\n",
    "    # sa_output1 = engine1.run()\n",
    "    engine2 = ConvEngineMatmul(input_tensor, kernels, stride=stride, tile_size=32, verbose=True)\n",
    "    sa_output2 = engine2.run()\n",
    "    # engine3 = ConvEngineSA(input_tensor, kernels, stride=stride, tile_size=32, verbose=True)\n",
    "    # sa_output3 = engine3.run()\n",
    "\n",
    "\n",
    "    # Golden convolution\n",
    "    def conv2d_stride(input_tensor, kernel, stride=1):\n",
    "        H, W, C = input_tensor.shape\n",
    "        R, S, _, K = kernel.shape\n",
    "        out_H = (H - R)//stride + 1\n",
    "        out_W = (W - S)//stride + 1\n",
    "        out = np.zeros((out_H, out_W, K))\n",
    "        for k in range(K):\n",
    "            for i in range(out_H):\n",
    "                for j in range(out_W):\n",
    "                    patch = input_tensor[i*stride:i*stride+R, j*stride:j*stride+S, :]\n",
    "                    out[i,j,k] = np.sum(patch * kernel[:,:,:,k])\n",
    "        return out\n",
    "\n",
    "    golden = conv2d_stride(input_tensor, kernels, stride=stride)\n",
    "\n",
    "    # print(\"SA Simulator Output Shape:\", sa_output1.shape)\n",
    "    # print(\"Golden Conv Shape:\", golden.shape)\n",
    "    # for k in range(sa_output1.shape[2]):\n",
    "    #     diff = np.max(np.abs(sa_output1[:,:,k] - golden[:,:,k]))\n",
    "    #     print(f\"Kernel {k} max absolute difference: {diff}\")\n",
    "\n",
    "    # # Print some sample values for debugging\n",
    "    # print(f\"\\nSample outputs (kernel 0, first few positions):\")\n",
    "    # print(f\"SA output: {sa_output1[0:3, 0:3, 0]}\")\n",
    "    # print(f\"Golden:    {golden[0:3, 0:3, 0]}\")\n",
    "\n",
    "    print(\"Matmul Simulator Output Shape:\", sa_output2.shape)\n",
    "    print(\"Golden Conv Shape:\", golden.shape)\n",
    "    for k in range(sa_output2.shape[2]):\n",
    "        diff = np.max(np.abs(sa_output2[:,:,k] - golden[:,:,k]))\n",
    "        print(f\"Kernel {k} max absolute difference: {diff}\")\n",
    "\n",
    "    print(f\"\\nSample outputs (kernel 0, first few positions):\")\n",
    "    print(f\"SA output: {sa_output2[0:3, 0:3, 0]}\")\n",
    "    print(f\"Golden:    {golden[0:3, 0:3, 0]}\")\n",
    "    # print(f\"\\nSample outputs (kernel 1, first few positions):\")\n",
    "    # print(f\"SA output: {sa_output[0:3, 0:3, 1]}\")\n",
    "    # print(f\"Golden:    {golden[0:3, 0:3, 1]}\")\n",
    "    # print(f\"\\nSample outputs (kernel 2, first few positions):\")\n",
    "    # print(f\"SA output: {sa_output[0:3, 0:3, 2]}\")\n",
    "    # print(f\"Golden:    {golden[0:3, 0:3, 2]}\")\n",
    "    # print(f\"\\nSample outputs (kernel 3, first few positions):\")\n",
    "    # print(f\"SA output: {sa_output[0:3, 0:3, 3]}\")\n",
    "    # print(f\"Golden:    {golden[0:3, 0:3, 3]}\")\n",
    "    # print(\"SA New Simulator Output Shape:\", sa_output3.shape)\n",
    "    # print(\"Golden Conv Shape:\", golden.shape)\n",
    "    # for k in range(sa_output3.shape[2]):\n",
    "    #     diff = np.max(np.abs(sa_output3[:,:,k] - golden[:,:,k]))\n",
    "    #     print(f\"Kernel {k} max absolute difference: {diff}\")\n",
    "\n",
    "    # # Print some sample values for debugging\n",
    "    # print(f\"\\nSample outputs (kernel 0, first few positions):\")\n",
    "    # print(f\"SA output: {sa_output3[0:3, 0:3, 0]}\")\n",
    "    # print(f\"Golden:    {golden[0:3, 0:3, 0]}\")\n",
    "\n",
    "#tiling and loading kernel is correct, output shapes are matching, difference are around 200\n",
    "#next steps: look into psum how is it handled, rotation (transpose) of SA at the end of operation, \n",
    "        #need to check if toeplitz is being streamed column or row wise into SA - i need it to stream using this column-staggered Toeplitz streaming (with initial zeros)\n",
    "        #check where we need to collect psum - after a chunk? after a tile? -After a chunk â†’ accumulate psums internally (still incomplete).\n",
    "\t# â€¢\tAfter a kernel (all chunks done) â†’ psums are final for that kernel and tile region.\n",
    "\t# â€¢\tAfter a tile â†’ flush psums out, move to next tile.\n",
    "\n",
    "#next step: sanity check - try actual matmul see if it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[30 32 29]\n",
      " [28 28 35]\n",
      " [30 25 36]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def im2col_2d(input_tensor, kernel_shape, stride=1):\n",
    "    \"\"\"\n",
    "    Convert input tensor into Toeplitz / im2col matrix.\n",
    "    input_tensor: 2D numpy array (H x W)\n",
    "    kernel_shape: tuple (kH, kW)\n",
    "    stride: stride for convolution\n",
    "    Returns:\n",
    "        cols: 2D array (num_patches x kernel_size)\n",
    "    \"\"\"\n",
    "    H, W = input_tensor.shape\n",
    "    kH, kW = kernel_shape\n",
    "    out_H = (H - kH) // stride + 1\n",
    "    out_W = (W - kW) // stride + 1\n",
    "\n",
    "    cols = []\n",
    "    for i in range(0, out_H*stride, stride):\n",
    "        for j in range(0, out_W*stride, stride):\n",
    "            patch = input_tensor[i:i+kH, j:j+kW].reshape(-1)\n",
    "            cols.append(patch)\n",
    "    return np.array(cols)  # shape = (num_patches, kH*kW)\n",
    "\n",
    "def conv2d_matmul(input_tensor, kernel, stride=1):\n",
    "    \"\"\"\n",
    "    Convolution as matmul\n",
    "    \"\"\"\n",
    "    kH, kW = kernel.shape\n",
    "    kernel_flatten = kernel.reshape(-1)  # shape = (kH*kW,)\n",
    "    Tmat = im2col_2d(input_tensor, (kH, kW), stride)  # shape = (num_patches, kH*kW)\n",
    "    Y = Tmat @ kernel_flatten  # matmul -> shape (num_patches,)\n",
    "    \n",
    "    # Reshape output to 2D\n",
    "    out_H = (input_tensor.shape[0] - kH) // stride + 1\n",
    "    out_W = (input_tensor.shape[1] - kW) // stride + 1\n",
    "    return Y.reshape(out_H, out_W)\n",
    "\n",
    "# ======================\n",
    "# Example\n",
    "# ======================\n",
    "input_tensor = np.array([\n",
    "    [1, 0, 3, 1, 1],\n",
    "    [2, 2, 0, 3, 1],\n",
    "    [4, 4, 4, 3, 3],\n",
    "    [0, 0, 3, 1, 4],\n",
    "    [2, 1, 2, 1, 4]\n",
    "])\n",
    "\n",
    "kernel = np.array([\n",
    "    [0, 1, 2],\n",
    "    [0, 2, 3],\n",
    "    [2, 1, 2]\n",
    "])\n",
    "\n",
    "output = conv2d_matmul(input_tensor, kernel, stride=1)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
